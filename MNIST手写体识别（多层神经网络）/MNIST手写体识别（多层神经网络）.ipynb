{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MNIST` 手写体识别是图像识别中最经典的问题，希望能够识别出人类手写的数字。数据是65000张灰度图和对对应的数字。用之前的深度神经网络来尝试解决这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Tensorflow 已经把 mnist 数据集集成在 examples 里面了\n",
    "#在这里 import 数据输入的部分\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先导入 `mnist` 数据集\n",
    "\n",
    "**注意** 下载数据需要一定时间，如果下面这行代码下载出现问题，可以从网上下载 MNIST 数据集，然后一起放到 `MNIST_data` 这个文件夹中，文件夹的结构应该是下面这样：\n",
    "```\n",
    "    MNIST_data\n",
    "        train-images-idx3-ubyte.gz\n",
    "        train-labels-idx1-ubyte.gz\n",
    "        t10k-images-idx3-ubyte.gz\n",
    "        t10k-labels-idx1-ubyte.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8a05813a8d81>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\WORKING\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\WORKING\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\WORKING\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\WORKING\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\WORKING\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到 `read_data_sets` 的一个参数是 `one_hot`([独热编码](https://baike.baidu.com/item/独热编码/9717350?fr=aladdin))\n",
    "\n",
    "它是识别任务中非常重要的一个概念，将一个数值 `n` 映射到一个向量，这个向量的第 `n` 个元素是 `1` ，其他元素都是 `0`。\n",
    "\n",
    "这个书局街分成了两个部分：训练和测试。分开来是为了观察模型在完全没有见过的数据上的表现，从而体现泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体看看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(ncols = 6, nrows = 2)\n",
    "plt.tight_layout(w_pad = -2.0, h_pad = -8.0)\n",
    "\n",
    "# 调用next_batch方法来一次性获取12个样本\n",
    "# 这里有一个‘shuffle’参数，表达是否打乱样本间的顺序\n",
    "images, labels = train_set.next_batch(12, shuffle = False)\n",
    "\n",
    "for ind, (image, label) in enumerate(zip(images, labels)):\n",
    "    # image 是一个 784 维的向量，是图片进行拉伸产生的，这里把它 reshape 回去\n",
    "    image = image.reshape((28, 28))\n",
    "    \n",
    "    # label 是一个 10 维的向量，哪个下标处的值为 1 说明是数字几（独热编码）\n",
    "    label = label.argmax()\n",
    "    \n",
    "    row = ind // 6\n",
    "    col = ind % 6\n",
    "    axes[row][col].imshow(image, cmap = 'gray') #灰度图\n",
    "    axes[row][col].axis('off')\n",
    "    axes[row][col].set_title('%d' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义深度网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(layer_input, output_depth, scope = 'hidden_layer', reuse = None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse = reuse):\n",
    "        # 注意这里的初始方法是truncated_normal\n",
    "        w = tf.get_variable(initializer = tf.truncated_normal_initializer(stddev = 0.1), \n",
    "                            shape = (input_depth, output_depth), name = 'weights')\n",
    "        \n",
    "        # 注意这里用 0.1 对偏置进行初始化\n",
    "        b = tf.get_variable(initializer = tf.constant_initializer(0.1), shape = (output_depth), name = 'bias')\n",
    "        net = tf.matmul(layer_input, w) + b\n",
    "        \n",
    "        return net\n",
    "    \n",
    "    \n",
    "def DNN(x, output_depths, scope = 'DNN', reuse = None):\n",
    "    net = x\n",
    "    \n",
    "    for i, output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net, output_depth, scope = 'layer%d' % i, reuse = reuse)\n",
    "        #注意这里的激活函数\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "    # 数字分为 0, 1, ..., 9 所以这是10分类问题\n",
    "    # 对应于 one_hot 的标签，所以这里输出一个 10维 的向量\n",
    "    net = hidden_layer(net, 10, scope = 'classification', reuse = reuse)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义占位符\n",
    "\n",
    "input_ph = tf.placeholder(shape = (None, 784), dtype = tf.float32)\n",
    "label_ph = tf.placeholder(shape = (None, 10), dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个5层的神经网络，每层的节点分别为：784，400，200，100，10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个分类问题，因此采用交叉熵来计算损失函数\n",
    "loss = tf.losses.softmax_cross_entropy(logits = dnn, onehot_labels = label_ph)\n",
    "\n",
    "# 下面定义的是正确率，注意理解它为什么是这么定义的\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis = -1), tf.argmax(label_ph, axis = -1)), dtype = tf.float32))\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 1.485448 train_acc: 0.609375 test_loss: 1.202393  test_acc: 0.765625\n",
      "STEP 2000: train_loss: 0.856910 train_acc: 0.765625 test_loss: 0.718500  test_acc: 0.859375\n",
      "STEP 3000: train_loss: 0.620737 train_acc: 0.812500 test_loss: 0.440792  test_acc: 0.921875\n",
      "STEP 4000: train_loss: 0.590660 train_acc: 0.828125 test_loss: 0.467028  test_acc: 0.843750\n",
      "STEP 5000: train_loss: 0.478044 train_acc: 0.906250 test_loss: 0.551444  test_acc: 0.890625\n",
      "STEP 6000: train_loss: 0.265536 train_acc: 0.937500 test_loss: 0.489413  test_acc: 0.843750\n",
      "STEP 7000: train_loss: 0.406452 train_acc: 0.921875 test_loss: 0.457152  test_acc: 0.875000\n",
      "STEP 8000: train_loss: 0.418945 train_acc: 0.859375 test_loss: 0.286238  test_acc: 0.937500\n",
      "STEP 9000: train_loss: 0.299432 train_acc: 0.875000 test_loss: 0.342349  test_acc: 0.890625\n",
      "STEP 10000: train_loss: 0.442951 train_acc: 0.843750 test_loss: 0.312232  test_acc: 0.906250\n",
      "STEP 11000: train_loss: 0.238033 train_acc: 0.953125 test_loss: 0.221827  test_acc: 0.968750\n",
      "STEP 12000: train_loss: 0.374870 train_acc: 0.890625 test_loss: 0.235831  test_acc: 0.875000\n",
      "STEP 13000: train_loss: 0.377174 train_acc: 0.859375 test_loss: 0.235142  test_acc: 0.937500\n",
      "STEP 14000: train_loss: 0.245321 train_acc: 0.937500 test_loss: 0.440816  test_acc: 0.875000\n",
      "STEP 15000: train_loss: 0.437233 train_acc: 0.937500 test_loss: 0.319605  test_acc: 0.906250\n",
      "STEP 16000: train_loss: 0.437108 train_acc: 0.890625 test_loss: 0.266460  test_acc: 0.921875\n",
      "STEP 17000: train_loss: 0.120408 train_acc: 0.937500 test_loss: 0.295525  test_acc: 0.906250\n",
      "STEP 18000: train_loss: 0.157266 train_acc: 0.953125 test_loss: 0.468603  test_acc: 0.906250\n",
      "STEP 19000: train_loss: 0.394347 train_acc: 0.890625 test_loss: 0.276103  test_acc: 0.875000\n",
      "STEP 20000: train_loss: 0.176175 train_acc: 0.953125 test_loss: 0.302890  test_acc: 0.937500\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.254883\n",
      "Train accuracy: 0.925745\n",
      "Test loss: 0.248610\n",
      "Test accuracy: 0.928400\n"
     ]
    }
   ],
   "source": [
    "# 训练20000次\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    \n",
    "    # 获取 batch_size 个训练样本\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict = {input_ph: images, label_ph: labels})\n",
    "    \n",
    "    if e % 1000 == 999:\n",
    "        \n",
    "        # 获取 batch_size 个测试样本\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        \n",
    "        # 计算在当前样本上的训练以及测试样本的损失值和正确率\n",
    "        loss_train, acc_train = sess.run([loss, acc], feed_dict = {input_ph: images, label_ph: labels})\n",
    "        loss_test, acc_test = sess.run([loss, acc], feed_dict = {input_ph: test_imgs, label_ph: test_labels})\n",
    "        \n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f}  test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "              \n",
    "print('Train Done!')\n",
    "print('-' * 30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict = {input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "    \n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失之以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict = {input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "    \n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，最后在训练集和测试集合上都达到了大约0.9的正确率，是一个相当不错的成绩了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard & `tf.summary`\n",
    "\n",
    "前面已经介绍过如何使用 `Tensorboard` 将构建的计算图显示出来，这里还有介绍它和 `tf.summary` 结合起来体现的更加强大的功能：可视化训练。\n",
    "\n",
    "首先介绍一下 `tf.summary` ，它能够手机训练过程中的各种 `tensor` 的信息并把它保存起来以供 `Tensorboard` 读取并展示。按照下面的方法来使用它：\n",
    "\n",
    "# 构造 `summary`\n",
    "\n",
    "---         \n",
    "* 如果想收集表示一个标量或者一个数的 `tensor` 的信息，比如上面的 `loss`    \n",
    "```python\n",
    "loss_sum = tf.summary.scalar('loss', loss)\n",
    "```     \n",
    "上面的语句就会告诉 `Tensorflow` ，在运行过程中，要让 `Tensorboard` 显示误差的变化了\n",
    "\n",
    "---\n",
    "* 如果想收集一个 `tensor` 的分布情况，这个 `tensor` 可以是任意形状，比如定义了一个 `(784, 400)` 的权重 `w`\n",
    "```python\n",
    "w_hist = tf.summary.histogram('w_hist', w)\n",
    "```\n",
    "---\n",
    "* 如果想收集一个4维的1-通道（灰度图），3-通道（RGB），4-（RGBA）的 `tensor` 的变化，比如输出了一个 `(1, 8, 8, 1)` 的灰度图 `image`\n",
    "```python\n",
    "image_sum = tf.summary.image('image', image)\n",
    "```\n",
    "---\n",
    "* 如果想收集一个3维(batch, frame, channel)，2维(batch, frame)的变化，比如输出了一个 `(10, 50, 3)` 的 `tensr` : `audio`\n",
    "```python\n",
    "audio_sum = tf.summary.audio('audio', audio)\n",
    "```\n",
    "---\n",
    "在本篇介绍中，暂时先使用 `scalar` 和 `histogram` 的 `summary` ， `image` 和 `audio` 的 `summary` 将在之后再介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#重新定义占位符\n",
    "input_ph = tf.placeholder(shape = (None, 784), dtype = tf.float32)\n",
    "label_ph = tf.placeholder(shape = (None, 10), dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，需要重新构建前向神经网络，为了简化代码，在构造一个隐藏层以及它的参数的函数内部构造 `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造权重，用 `truncated_normal` 初始化\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 构造偏置，用 ‘0.1’ 初始化\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造添加 ‘variable’ 的 ‘summary’ 的函数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        \n",
    "        #计算平均值\n",
    "        mean = tf.reduce_mean(var)\n",
    "        \n",
    "        #将平均值添加到 ‘summary‘ 中，这是一个数值，所以用 'tf.summary.scalar'\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        # 计算标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            \n",
    "        # 将标准差添加到 ‘summary’ 中\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        \n",
    "        # 添加最大值，最小值 ‘summary’\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        \n",
    "        # 添加这个变量分布情况的 ’summary‘， 希望观察它的分布，所以用’tf.summary.histogram‘\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个隐藏层\n",
    "def hidden_layer(x, output_dim, scope = 'hidden_layer', act = tf.nn.relu, reuse = None):\n",
    "    input_dim = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.name_scope(scope):\n",
    "        with tf.name_scope('weight'):\n",
    "            # 构造 ‘weight；\n",
    "            weight = weight_variable([input_dim, output_dim])\n",
    "            # 添加 ‘weight’ 的 ‘summary’\n",
    "            variable_summaries(weight)\n",
    "            \n",
    "        with tf.name_scope('bias'):\n",
    "            # 构造 'bias'\n",
    "            bias = bias_variable([output_dim])\n",
    "            # 添加 ‘bias’ 的 ‘summary’\n",
    "            variable_summaries(bias)\n",
    "            \n",
    "        with tf.name_scope('linear'):\n",
    "            # 计算 ‘x * w + b’\n",
    "            preact = tf.matmul(x,  weight) + bias\n",
    "            # 添加激活层之前输出的分布情况‘summary’\n",
    "            tf.summary.histogram('pre_activation', preact)\n",
    "            \n",
    "        # 经过激活层 ‘act’\n",
    "        output = act(preact)\n",
    "        # 添加激活后输出的分布情况到 ‘summary’\n",
    "        tf.summary.histogram('output', output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "# 构造深度神经网络\n",
    "def DNN(x, output_depths, scope = 'DNN_with_sums', reuse = None):\n",
    "    with tf.name_scope(scope):\n",
    "        net = x\n",
    "        for i, output_depth in enumerate(output_depths):\n",
    "            net = hidden_layer(net, output_depth, scope = 'hidden%d' % (i + 1), reuse = reuse)\n",
    "            \n",
    "        #最后一个分类层.\n",
    "        #其中，有点想不明白参数 ‘act=tf.identity’ 的作用，\n",
    "        #难道是让上面定义hidden_layer时定义好的 ‘act = tf.nn.relu’ 不执行吗，\n",
    "        #因为下面 ‘tf.losses.softmax_cross_entropy’ 自带softmax激活函数，所以不让最后一层linear在输出之前进入激活函数，\n",
    "        net = hidden_layer(net, 10, scope = 'classification', act = tf.identity, reuse = reuse)\n",
    "        return net\n",
    "    \n",
    "dnn_with_sums = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义 ‘loss’，‘acc’，‘train_op’\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.losses.softmax_cross_entropy(logits = dnn_with_sums, onehot_labels = label_ph)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn_with_sums, axis = -1), tf.argmax(label_ph, axis = -1)), \n",
    "                                 dtype = tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    lr = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （可选）融合 `summary`\n",
    "- - -\n",
    "- 我们可以把前面定义的所有`summary`都融合成一个`summary`\n",
    "```python\n",
    "merged = tf.summary.merge_all()\n",
    "```\n",
    "- - -\n",
    "- 也可以只是融合某些`summary`\n",
    "```python\n",
    "merged = tf.summary.merge([loss_sum, image_sum])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出 `summary`\n",
    "---\n",
    "`summary`是需要导出到外部文件的\n",
    "- 首先定义一个文件读写器\n",
    "```python\n",
    "summary_writer = tf.summary.FileWriter('summaries', sess.graph)\n",
    "```\n",
    "- - -\n",
    "- 然后在训练的过程中, 在你希望的时候运行一次`merged`或者是你之前自己定义的某个通过`summary`定义的`op`\n",
    "```python\n",
    "summaries = sess.run(merged, feed_dict={...})\n",
    "```\n",
    "- - -\n",
    "- 然后将这个`summaries`写入到`summari_writer`内\n",
    "```python\n",
    "summary_writer.add_summary(summaries, step)\n",
    "```\n",
    "注意`step`表示你当前训练的步数, 当然你也可以设定为其他你想要用的数值\n",
    "\n",
    "- - -\n",
    "- 最后关闭文件读写器\n",
    "```python\n",
    "summary_writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('test_summary/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('test_summary/test', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 0.430697 train_acc: 0.843750 test_loss: 0.355455 test_acc: 0.906250\n",
      "STEP 2000: train_loss: 0.140123 train_acc: 0.968750 test_loss: 0.401140 test_acc: 0.875000\n",
      "STEP 3000: train_loss: 0.167068 train_acc: 0.937500 test_loss: 0.258671 test_acc: 0.906250\n",
      "STEP 4000: train_loss: 0.145217 train_acc: 0.953125 test_loss: 0.096235 test_acc: 0.968750\n",
      "STEP 5000: train_loss: 0.147932 train_acc: 0.968750 test_loss: 0.080337 test_acc: 0.984375\n",
      "STEP 6000: train_loss: 0.110093 train_acc: 0.984375 test_loss: 0.040668 test_acc: 1.000000\n",
      "STEP 7000: train_loss: 0.093921 train_acc: 0.968750 test_loss: 0.084026 test_acc: 0.984375\n",
      "STEP 8000: train_loss: 0.077281 train_acc: 0.984375 test_loss: 0.129494 test_acc: 0.937500\n",
      "STEP 9000: train_loss: 0.062933 train_acc: 0.968750 test_loss: 0.212640 test_acc: 0.906250\n",
      "STEP 10000: train_loss: 0.029841 train_acc: 1.000000 test_loss: 0.098628 test_acc: 0.984375\n",
      "STEP 11000: train_loss: 0.058552 train_acc: 0.984375 test_loss: 0.095477 test_acc: 0.968750\n",
      "STEP 12000: train_loss: 0.036131 train_acc: 1.000000 test_loss: 0.154863 test_acc: 0.968750\n",
      "STEP 13000: train_loss: 0.083856 train_acc: 0.968750 test_loss: 0.058559 test_acc: 0.968750\n",
      "STEP 14000: train_loss: 0.028938 train_acc: 0.984375 test_loss: 0.081626 test_acc: 0.984375\n",
      "STEP 15000: train_loss: 0.020857 train_acc: 0.984375 test_loss: 0.125552 test_acc: 0.968750\n",
      "STEP 16000: train_loss: 0.043664 train_acc: 0.984375 test_loss: 0.056093 test_acc: 0.968750\n",
      "STEP 17000: train_loss: 0.050445 train_acc: 0.984375 test_loss: 0.047728 test_acc: 0.968750\n",
      "STEP 18000: train_loss: 0.032922 train_acc: 1.000000 test_loss: 0.160114 test_acc: 0.968750\n",
      "STEP 19000: train_loss: 0.014279 train_acc: 1.000000 test_loss: 0.217930 test_acc: 0.953125\n",
      "STEP 20000: train_loss: 0.048653 train_acc: 0.984375 test_loss: 0.099096 test_acc: 0.953125\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.052347\n",
      "Train accuracy: 0.984800\n",
      "Test loss: 0.099086\n",
      "Test accuracy: 0.970500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict = {input_ph: images, label_ph: labels})\n",
    "    \n",
    "    if e % 1000 == 999:\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 获取 ‘train’ 数据的 ‘summaries’ 以及 ’loss‘，’acc‘ 信息\n",
    "        sum_train, loss_train, acc_train = sess.run([merged, loss, acc], feed_dict = {input_ph: images, label_ph: labels})\n",
    "        \n",
    "        # 将 ‘train' 数据的 ’summaries‘ 写入到 ’train_writer‘ 中\n",
    "        train_writer.add_summary(sum_train, e)\n",
    "        \n",
    "        # 获取 ’test‘ 数据的 ’summaries‘ 以及 ’loss‘，‘acc’ 信息\n",
    "        sum_test, loss_test, acc_test = sess.run([merged, loss, acc], feed_dict = {input_ph: test_imgs,  label_ph: test_labels})\n",
    "        \n",
    "        # 将 'test' 的 ’summaries‘ 写入到 ’test_writer' 中\n",
    "        test_writer.add_summary(sum_test, e)\n",
    "        \n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "# 关闭读写器\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "print('Train Done!')\n",
    "print('-' * 30)\n",
    "\n",
    "# 计算所有训练样本的损失率以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict = {input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "    \n",
    "    \n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失之以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict = {input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "    \n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开`Tensorboard`\n",
    "在之前对计算图可视化的时候, 我们用`tensorboard --logdir=.`命令打开过`Tensorboard`显示当前目录下 `.` 文件，但`Tensorboard`支持打开多个目录下的`.events`文件, 方便我们对比不同模型或者训练和测试之间的差别\n",
    "\n",
    "\n",
    "在`test_summary`目录中输入以下命令\n",
    "```\n",
    "$ tensorboard --logdir=train:train/,test:test/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
